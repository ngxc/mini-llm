# mini-llm  
一个简易的 LLM 模型  by ngxc  
相比minimind更直观的大模型0到1构建项目

---

##  项目简介
本项目模型仅供学习，旨在**对大模型祛魅**。  
让你能最简单的看到大模型的结构以及最简单的训练。

让你了解：大语言模型的核心其实只是一个**预测器**。  

它会根据“上文 + 因果 + 注意力”去预测下一个 token 的概率。  
这就是大模型的本质。
![img_2.png](https://youke1.picui.cn/s1/2025/11/04/69096a912735d.png)
---

##  模型原理简述
Transformer 的注意力机制通过大量上下文预训练，学习了语义关联。  
而**预训练（Pretrain）**的作用主要有两个：
1. **学会字与句的关系**  
   例：  
   > “我觉得 A 是一个不错的人，他为人慷慨。”  
   模型能理解“他”与“A”指向同一个实体，这是注意力机制建立的代词指代关系。  

2. **学习通用知识**  
   例如建立“马里亚纳海沟 ⇋ 最深海沟”的知识联系。  

预训练后模型只能“接龙”，还不会完整地对话。  
为了让模型学会“根据问题输出答案”，还需要进行 **SFT（监督微调）**。

---

##  数据集
为了便于训练，本项目的数据集均清洗至 **512 tokens** 长度，并添加了特殊 tokens。  

| 阶段 | 描述 | 下载链接 |
|---|---|---|
| 预训练 | 中文 Wiki 文本，已清洗 | [🔗 ngxc11/mini-llm-dataset](https://www.modelscope.cn/datasets/ngxc11/mini-llm-dataset/files) |
| SFT | 单轮问答对，已加特殊 token | [🔗 gongjy/minimind_dataset](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files) |

---

##  模型结构
本项目使用 **稠密 Transformer** 架构，提供两种规格：

| 型号 | 宽度 | 深度 | 隐藏维度 | 特点 |
|---|---|---|---|---|
| **矮胖型** | 512 | 8 | 512 | 默认配置，训练更快 |
| **瘦高型** | 512 | 24 | 512 | 性能更强，需添加残差防止梯度爆炸 |



- 分词器：`bert-base-chinese`  
  每个汉字都有对应的 ID，便于模型输入处理。  
- 经过验证，深层网络效果更好，但需防止梯度爆炸。  
- MoE 版本正在训练中，预计训练完成后开源。  
- 如果要自己修改可以在confid类里修改

- ![img_3.png](https://youke1.picui.cn/s1/2025/11/04/69096aad8fa0e.png)
---

##  使用说明

### 1. 预训练
当预训练结束后可以通过chat来观察，可以发现模型只是会接龙，如果要最快完成项目一轮即可，如果想要比较合理的接龙效果建议3轮训练。
```bash
# 下载预训练数据集 wiki_zh_texts_filtered.json
python pre_train.py        # 开始训练（默认矮胖模型）
# 可在 config中修改为瘦高模型

# RTX 3090 约需 13 小时
python pre_chat.py         # 接龙测试模型效果
```
####  可以看到预训练后的模型能顺着我们的话语接着往下输出但是逻辑不算流畅，这个算是正常的
![img.png](https://youke1.picui.cn/s1/2025/11/04/69096a57407b9.png)
### 2.sft微调
完成这一步模型算是可以对话的程度，可以使用sft_chat观察模型的效果，为了避免遗忘预训练的知识，这里建议学习率为预训练的1/5。一般一轮即可
```bash
# 去minimind的仓库下载512长度的sft数据集，大小都可以
python sft_train.py
# 在config里修改模型保证与第一步模型同样的结构
python sft_chat.py
```
#### 可以看到模型能根据我们的问题回答问题了，我们从0到1训练了一个自己的模型
![img_1.png](https://youke1.picui.cn/s1/2025/11/04/69096a78b708a.png)
### 3.说明
两个模型只作为基础模型，可以自己修改参数变成更大规模的模型

---

## 后续有moe的版本请敬请期待。
